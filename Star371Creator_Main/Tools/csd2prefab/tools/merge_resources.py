#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è³‡æºåˆä½µå·¥å…· - å°‡ output_resources.json èˆ‡ input_resources.json åˆä½µæˆæ–°çš„ input_resources.json
"""

import json
import time
from pathlib import Path


def merge_resources(output_resources_path="output_resources.json", input_resources_path="input_resources_backup.json", merged_output_path="input_merged.json"):
    """
    åˆä½µå…©å€‹è³‡æºæ–‡ä»¶
    
    Args:
        output_resources_path: æ–°ç”Ÿæˆçš„è³‡æºæ–‡ä»¶è·¯å¾‘
        input_resources_path: åŽŸæœ‰çš„è³‡æºæ–‡ä»¶è·¯å¾‘  
        merged_output_path: åˆä½µå¾Œçš„è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
    """
    
    print(f"ðŸ”„ é–‹å§‹åˆä½µè³‡æºæ–‡ä»¶...")
    print(f"  ðŸ“ è¼¸å…¥æ–‡ä»¶1: {output_resources_path}")
    print(f"  ðŸ“ è¼¸å…¥æ–‡ä»¶2: {input_resources_path}")  
    print(f"  ðŸ“„ è¼¸å‡ºæ–‡ä»¶: {merged_output_path}")
    
    # è¼‰å…¥ output_resources.json (æ–°è³‡æº)
    output_data = {}
    if Path(output_resources_path).exists():
        print(f"âœ… è¼‰å…¥æ–°è³‡æºæ–‡ä»¶: {output_resources_path}")
        with open(output_resources_path, "r", encoding="utf-8") as f:
            output_data = json.load(f)
    else:
        print(f"âš ï¸  æ–°è³‡æºæ–‡ä»¶ä¸å­˜åœ¨: {output_resources_path}")
    
    # è¼‰å…¥ input_resources.json (èˆŠè³‡æº)  
    input_data = {}
    if Path(input_resources_path).exists():
        print(f"âœ… è¼‰å…¥èˆŠè³‡æºæ–‡ä»¶: {input_resources_path}")
        with open(input_resources_path, "r", encoding="utf-8") as f:
            input_data = json.load(f)
    else:
        print(f"âš ï¸  èˆŠè³‡æºæ–‡ä»¶ä¸å­˜åœ¨: {input_resources_path}")
    
    # å‰µå»ºåˆä½µå¾Œçš„è³‡æ–™çµæ§‹
    merged_data = {
        "version": "1.0",
        "timestamp": str(time.time()),
        "image_cache": {},
        "particle_cache": {},
        "font_cache": {},
        "csd_cache": {},
        "path_mapping": {}
    }
    
    # åˆä½µå„ç¨®ç·©å­˜ - æ–°è³‡æºå„ªå…ˆ
    cache_types = ["image_cache", "particle_cache", "font_cache", "csd_cache", "path_mapping"]
    
    for cache_type in cache_types:
        print(f"ðŸ“ åˆä½µ {cache_type}...")
        
        # å…ˆæ·»åŠ èˆŠè³‡æº
        if cache_type in input_data:
            merged_data[cache_type].update(input_data[cache_type])
            old_count = len(input_data[cache_type])
        else:
            old_count = 0
        
        # å†æ·»åŠ æ–°è³‡æº (è¦†è“‹é‡è¤‡çš„é …ç›®)
        if cache_type in output_data:
            merged_data[cache_type].update(output_data[cache_type])
            new_count = len(output_data[cache_type])
        else:
            new_count = 0
            
        merged_count = len(merged_data[cache_type])
        print(f"  ðŸ”¢ èˆŠ: {old_count}, æ–°: {new_count}, åˆä½µå¾Œ: {merged_count}")
    
    # ä¿å­˜åˆä½µçµæžœ
    print(f"ðŸ’¾ ä¿å­˜åˆä½µçµæžœåˆ°: {merged_output_path}")
    with open(merged_output_path, "w", encoding="utf-8") as f:
        json.dump(merged_data, f, indent=2, ensure_ascii=False)
    
    # çµ±è¨ˆå ±å‘Š
    total_old = sum(len(input_data.get(cache_type, {})) for cache_type in cache_types)
    total_new = sum(len(output_data.get(cache_type, {})) for cache_type in cache_types)  
    total_merged = sum(len(merged_data[cache_type]) for cache_type in cache_types)
    
    print(f"")
    print(f"ðŸ“Š åˆä½µçµ±è¨ˆ:")
    print(f"  ðŸ—‚ï¸  èˆŠè³‡æºç¸½æ•¸: {total_old}")
    print(f"  ðŸ†• æ–°è³‡æºç¸½æ•¸: {total_new}")
    print(f"  ðŸ“‹ åˆä½µå¾Œç¸½æ•¸: {total_merged}")
    print(f"  ðŸ”„ é‡è¤‡é …ç›®: {total_old + total_new - total_merged}")
    print(f"")
    print(f"âœ… è³‡æºåˆä½µå®Œæˆ!")


if __name__ == "__main__":
    import sys
    
    # é è¨­åƒæ•¸
    output_path = "output_resources.json"
    input_path = "input_resources_backup.json" 
    merged_path = "input_merged.json"
    
    # å¾žå‘½ä»¤è¡Œåƒæ•¸ç²å–è·¯å¾‘ (å¯é¸)
    if len(sys.argv) > 1:
        output_path = sys.argv[1]
    if len(sys.argv) > 2:
        input_path = sys.argv[2]
    if len(sys.argv) > 3:
        merged_path = sys.argv[3]
    
    merge_resources(output_path, input_path, merged_path)
